---
title: "pre_lab_12.Rmd"
author: "Adelia McGuire"
date: "2024-11-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# An intro to text analysis

Throughout this course, we've been focused on finding information in structured data. We've learned a lot of techniques to do that, and we've learned how the creative mixing and matching of those skills can find new insights.

What happens when the insights are in unstructured data? Like a block of text?

Turning unstructured text into data to analyze is a whole course in and of itself -- and one worth taking if you've got the credit hours -- but some simple stuff is in the grasp of basic data analysis.

To do this, we'll need a new library -- [tidytext](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html), which you can guess by the name plays very nicely with the tidyverse. So install it in the console with `install.packages("tidytext")` and we'll get rolling.

### Task 1: Load libraries and settings

**Task** Run the following code in the gray-colored codeblock below to load the libraries we'll use.

```{r}
#install.packages("tidytext")
library(tidyverse)
library(tidytext)
library(janitor)
library(lubridate)
library(rvest)
```

Remember the end of the previous lab, where you scraped all of those press release links from Maryland Sen. Ben Cardin's website? We're going to take that one step further and analyze the actual text of those releases using tidytext. Our starting question: what words or phrases appear most in Cardin's press releases?

To answer this question, we'll use the text of those releases. For the scraping lab we gathered the URLs of 999 of Cardin's press releases, and for this exercise we'll be working with the text of them.

Let's read in this data and examine it:

### Task 2: Read in data

**Task** Run the following code and describe the dataframe it outputs.

**Answer** The following code outputs a dataframe titled "Releases" that consists of 999 rows, corresponding to the 999 press releases of Senator Ben Cardin that we scraped in Lab 11. It provides four key pieces of information in the form of  four columns (date, title, url, and text). The date column provides information on the date when the press release was published, the title column contains the title or headline of the press release, the URL column has each of the URL's where the full press release can be accessed, and finally, the only addition to this dataframe, the text column provides the full text content of the press release itself.

```{r}
releases <- read_rds("data/cardin_releases.rds")
```

As an aside, below is an example of how you would scrape the text from the first 10 press releases. We already have the text in the dataframe, but this lets you know how it was collected.

### Task 3: Example of gathering text

**Task** Run the following code to gather the text for the first 10 press releases.

```{r}

urls <- releases |> top_n(10) |> pull(url)

release_text <- tibble(url = character(), text = character())

# loop over each url in the list of urls
for (u in urls){
  # wait a fraction of a second so we don't hammer the server
  Sys.sleep(0.2)
  # read in the html from the url
  html <- u |> read_html()
  # use the xpath of the text of the release to grab it and call html_text() on it
  text <- html |> 
    html_element(xpath="/html/body/div/div/div/div/div/div/div[2]/div[1]/div/div[4]") |> 
    html_text()
  
  release_text <- release_text |> add_row(url = u, text = str_squish(text))
}

release_text
```

What we want to do is to make the `text` column easier to analyze. Let's say we want to find out the most commonly used words. We'll want to remove URLs from the text of the releases since they aren't actual words. Let's use mutate to make that happen:

### Task 4: Remove URLs from content

**Task** Run the following code.

```{r}
releases <- releases |>
  mutate(text = gsub("http.*","", text))
```

If you are trying to create a list of unique words, R will treat differences in capitalization as unique and also will include punctuation by default, even using its `unique` function:

### Task 5: Trying out unique

**Task** Run the following code and describe what the `unique` function does to the original list of words. 

**Answer** The 'unique' function, as displayed below, allows us to create a list of unique words. It is used to return a vector of unqiue elements and in doing so removes duplicate values. The unique function is like another way to write distinct. We are using the unique function to ensure that the list of words ("Dog", "dog", "dog", "cat", "cat", ",") only appear once. It is important to note that this function does treat capitalized and lowercase versions of the same word as two disctint words, hence why were are seeing "Dog" and "dog" treated as two unique words in the output. The same is true in how this function treats punctuation marks... in this case, the comma "," is considered a unique element. The output that we are seeing is a result of the unique() function processing  a_list_of_words and removing any repeated elements. 

```{r}
a_list_of_words <- c("Dog", "dog", "dog", "cat", "cat", ",")
unique(a_list_of_words)
```

Fortunately, this is a solved problem with tidytext, which has a function called `unnest_tokens` that will convert the text to lowercase and remove all punctuation. The way that `unnest_tokens` works is that we tell it what we want to call the field we're creating with this breaking apart, then we tell it what we're breaking apart -- what field has all the text in it. For us, that's the `text` column:

### Task 6: Trying out unnest_tokens

**Task** Run the following code and describe the output of using the `unnest_tokens` function. 

**Answer**  The unnest_tokens function from the tidytext package is used to break apart text into individual words while also simultaneously cleaning up the text by converting everything to lowercase and removing punctuation. Basically it is fixing the exact potential issues I identified in my response to Task 5. As you can see in the output the unnest_tokens function is fairly agnostic in what it considers is and is not a "word". The changes that we are seeing here are... Punctuation ( commas and exclamation marks) are removed, capitalization is ignored, so two words that are the same other than capitalization are treated as the same word, and each word is listed separately in the new word column. The output from using the unnest_tokens function is a dataframe called unique_words that contains a column of individual words from the original text column.

```{r}
unique_words <- releases |> select(text) |>
  unnest_tokens(word, text)
View(unique_words)
```

### Task 7: Make a column chart

**Task** Run the following code and describe what the resulting graphic shows. Is it interesting?

**Answer** The provided code creates a a bar plot that shows the frequency of the top 25 most common words (from Cardin's press releases) in the unique_words dataset. As said in class today, these top 25 words are far from riveting. According to the resulting graphic, "the", "and", and "to", are the top three words in these press releases. This plot, in my opinion, is not very interesting because it really only highlights stopwords that are fairly common in any text. While it is informative in its job of finding the frequency of words, the words that it identifies does not provide much insight into the specific topics/themes of the press releases.

Now we can look at the top words in this dataset. Let's limit ourselves to making a plot of the top 25 words, and we'll use the function `count` to do the counting:

```{r}
unique_words |>
  count(word, sort = TRUE) |>
  top_n(25) |>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in Cardin releases")
```

Well, that's a bit underwhelming - a lot of very common (and short) words. This also is a solved problem in working with text data, and words like "a" and "the" are known as "stop words". In most cases you'll want to remove them from your analysis since they are so common. Tidytext provides a dataframe of them that we'll load, and then we'll add some of our own.

### Task 8: Load the stop words

**Task** Run the following code

```{r}
data("stop_words")

stop_words <- stop_words |> 
  add_row(word = "ben") |> 
  add_row(word = "cardin") |> 
  add_row(word = "senator") |>
  add_row(word = "senators") |>
  add_row(word = "maryland") |>
  add_row(word = 'federal') |> 
  add_row(word = 'u.s') |> 
  add_row(word = 'md') |> 
  add_row(word = 'senate') |> 
  add_row(word = "hollen") |> 
  add_row(word = "van") |> 
  add_row(word = "chris") |> 
  add_row(word = "project") |> 
  add_row(word = "program") 

```

Then we're going to use a function we haven't used yet called an `anti_join`, which filters out any matches. So we'll `anti_join` the stop words and get a list of words that aren't stop words.

From there, we can get a simple word frequency by just grouping them together and counting them. We can borrow the percent code from above to get a percent of the words our top 10 words represent.

### Task 9: Using anti_join

**Task** Run the following code and describe the results. Is it more interesting than before? 

**Answer** The following code works to clean the word list by removing common stop words, words deemed as "lame" or unworthy of inclusion. An anti join, as seen in the code, is an exclusionary join, in this case it removes words that are undesired. These results are far more interesting than the former output. The words I am seeing here have some substance to them... for example "health", "support", and "act" are the top three frequent words. The words now reflect more meaningful topics that are likely relevant terms relevant that provide helpful information about which specific area Cardin chooses to put increased focus on. By removing stop words, we are presented with a far clearer view of the actual content within these press releases and it even allows for a deeper understanding of the key topics in the dataset.

```{r}
unique_words |>
  anti_join(stop_words) |>
  group_by(word) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |>
  top_n(10)
```

Those seem like more relevant unique words. Now, here's where we can start to do more interesting and meaningful analysis. Let's create two dataframes of unique words based on time: one for all of 2022 and the other for all of 2024:

### Task 10: Create dataframes for 2022 and 2024

**Task** Run the following code

```{r}
unique_words_2022 <- releases |>
  filter(year(date) == 2022) |>
  select(text) |>
  unnest_tokens(word, text)

unique_words_2024 <- releases |>
  filter(year(date) == 2024) |>
  select(text) |>
  unnest_tokens(word, text)
```

Then we can create top 10 lists for both of them and compare:

### Task 11: Create dataframes with the top 10 words in each year

**Task** Run the following code and describe the results. 

**Answer** This code creates two separate dataframes, each of which contains the top 10 non-stop words from the press releases in the years 2022 and 2024. These two separate new dataframes are subsets of the whole compilation of 999 press releases. When comparing the two dataframes it is clear that the most frequent words in 2022 are very similar to those in 2024. Cardin's use of words like health, support, policy, care, and reform appear in both years, suggesting that the main topics of focus in the press releases remain consistent over the two years. I am not very surprised to find this lack of difference because while yes language can evolve, it often does not undergo drastic change in a short period of two years. I find this analysis to be far more interesting than the previous analysis that included stop word because this one  focuses on meaningful words. I also appreciated how it provides insight into ongoing issues and Cardin's policy priorities by comparing his language used in press releases across the span of two years

```{r}
unique_words_2022 |>
  anti_join(stop_words) |>
  group_by(word) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |>
  top_n(10)

unique_words_2024 |>
  anti_join(stop_words) |>
  group_by(word) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |>
  top_n(10)
```

In the 2022 top 10 list, "health" is first, which makes some sense, while the 2024 list leads with "baltimore".

## Going beyond a single word

The next step in text analysis is using `ngrams`. An `ngram` is any combination of words that you specify. Two word ngrams are called bigrams (bi-grams). Three would be trigrams. And so forth.

The code to make ngrams is similar to what we did above, but involves some more twists.

So this block is is going to do the following:

1.  Use the releases data we created above, and filter for 2022 releases.
2.  Unnest the tokens again, but instead we're going to create a field called bigram, break apart summary, but we're going to specify the tokens in this case are ngrams of 2.
3.  We're going to make things easier to read and split bigrams into word1 and word2.
4.  We're going to filter out stopwords again, but this time we're going to do it in both word1 and word2 using a slightly different filtering method.
5.  Because of some weirdness in calculating the percentage, we're going to put bigram back together again, now that the stop words are gone.
6.  We'll then group by, count and create a percent just like we did above.
7.  We'll then use top_n to give us the top 10 bigrams.

### Task 12: Create a dataframe with the top 10 two-word phrases for 2022

**Task** Run the following code and describe the results. 

**Answer** This code is going a step further from what we have been doing by identifying the top 10 most frequent bigrams (two-words) in Cardin's press releases from 2022. Beyond containing the top 10 most frequent bigrams from the 2022 press releases, the output also displays each of the words frequencies and percent. 
Overall, because we are looking at two-word phrases instead of singular words we have the ability to access nuanced meanings and understand the specific contect in which words appear. For example, "health care" and "mental health" together provide more insight than just the word "health" alone. Considering we are now taking two words not just one like we had previously, these words are more specific and provide more context to what he is referring to. For example, since 2022 was the year of COVID-19, their are quite a few set of words that pertain to the pandemic. The inclusion of "covid 19" and "health care" indicates to me as a reader that the COVID-19 pandemic had substantial influence on the conversations and actions discussed in the press releases.

```{r}
releases |>
  filter(year(date) == 2022) |>
  unnest_tokens(bigram, text, token = "ngrams", n = 2) |>
  separate(bigram, c("word1", "word2"), sep = " ") |>
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word) |>
  mutate(bigram = paste(word1, word2, sep=" ")) |>
  group_by(bigram) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |>
  top_n(10)
```

And we already have a different, more nuanced result. Health was among the top single words, and we can see that "health care", "human rights" and "chesapeake bay" are among the top 2-word phrases. What about 2024?

### Task 13: Create a dataframe with the top 10 two-word phrases for 2024

**Task** Run the following code and describe the results. 

**Answer** Now observing the frequency of two-word mentions in Ben Cardin's press releases from the year 2024, we are seeing some of the same words like "Baltimore city" and "Chesapeake bay" but also many different words. The frequency of word sets depends largely on what is important in a specific year. In 2022, as I said before, "covid 19" was one of the most frequent words sets since out nation was in the middle of a pandemic, but now in 2024 "location Baltimore" was included maybe due to the bridge collapse that occurred in Baltimore.This code works to give us back concepts and ideas, this is why we are seeing more sets of words such as "health care" or "human rights" rather than something basic and surface level like "is the" or "we will". 

```{r}
releases |>
  filter(year(date) == 2024) |>
  unnest_tokens(bigram, text, token = "ngrams", n = 2) |>
  separate(bigram, c("word1", "word2"), sep = " ") |>
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word) |>
  mutate(bigram = paste(word1, word2, sep=" ")) |>
  group_by(bigram) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |>
  top_n(10)
```

No more "covid 19", for one, but not a lot of changes otherwise. You'll notice that the percentages are very small; that's not irrelevant but in some cases it's the differences in patterns that's more important.

There are some potential challenges to doing an analysis. For one, there are variations of words that could probably be standardized - maybe using OpenRefine - that would give us cleaner results. There might be some words among our list of stop words that actually are meaningful in this context.

Or..... we could have R tackle this.

### Task 14: Install a package for stemming and lemmatizing words

**Task** Run the following code.

```{r}
install.packages("pacman") # comment this line out after you run it the first time
pacman::p_load_gh("trinker/textstem")
library(textstem)
```

[textstem](https://github.com/trinker/textstem) is a library that has two functions: stemming and lemmatization. What are they? Both of them convert a set of words or phrases into different forms, but the differences are worth seeing:

### Task 15: Stemming vs. Lemmatization

**Task** Run the following code and describe the differences between the two results. Which one might be more useful for the press release data? 

**Answer** Both stemming and lemmatization are ways in which a set of words and phrases can be converted in different forms. The difference u=between these two functions is in how they do this conversion... Stemming the following words will give you: "driver" "drive"  "drove"  "driven" "drive"  "drive". Legitimatizing these same words will give you: "driver" "drive"  "drive"  "drive"  "drive"  "drive". While each of these functions are useful in different ways, in this situation I would opt to go with Lemmatization. Lemmatization seems to be the more appropriate choice when dealing with a document such as a press releases because of the accuracy, readability, and just straight up consistency that it ensures. It works to reduce words to their proper forms and considering that that press releases strive to be formal and precise, I am confident that lemmatization would be the better choice.

```{r}
dw <- c('driver', 'drive', 'drove', 'driven', 'drives', 'driving')

stem_words(dw)

lemmatize_words(dw)

```

Let's try lemmatization on the 2024 unique words, and then compare the top ten to the original top ten from Task 11:

### Task 16: Lemmatization vs. Words

**Task** Run the following code and describe the differences between the two results. Which one is more useful? 

**Answer**  The legitimatized version gives you the more generic version of the word. The 2024 without lematization dataframe is primarily about money, containing words such as "fund", "support", "provide", "million". When observing this output it is clear that the lemmatized version contains fewer variations of the same word, allowing for the top 10 words to be more clean and consistent since relative forms are being grouped together. When it comes to deciding which is more useful its improtant to take into consideration what your task is... in any case where you are interested in tracking specific variations of words, the non-lemmatized version may be useful, but in most cases, lemmatization is the better choice.

```{r}
unique_words_2024_lemma <- lemmatize_words(unique_words_2024$word) |> as_tibble() |> rename(word = value)

unique_words_2024_lemma |>
  anti_join(stop_words) |>
  group_by(word) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |>
  top_n(10)

unique_words_2024 |>
  anti_join(stop_words) |>
  group_by(word) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |>
  top_n(10)

```
## Sentiment Analysis

Another popular use of text analysis is to measure the sentiment of a word - whether it expresses a positive or negative idea - and tidytext has built-in tools to make that possible. We use word counts like we've already calculated and bring in a dataframe of words (called a lexicon) along with their sentiments using a function called `get_sentiments`. The most common dataframe is called "bing" which has nothing to do with the Microsoft search engine. Let's load it:

### Task 17: Load the bing lexicon and produce sentiments for our 2022 and 2024 unique words

**Task** Run the following code and describe the results. Do any of the sentiments seem incorrect or counter-intuitive? 

**Answer** Sentiment analysis is the practice of measuring the sentiment of a word, attempting to decide if a specific work should be deemed as a "positive" or a "negative" word. When it comes to sentiment analysis context matters and I have found that nearly every single one of the words in this output can be argued and are very up for debate. While I get the general premise of sentiment analysis and am sure it can be helpful for data journalists in certain situations I am really just not a fan as I think it runs the risk of being very misleading and providing misinformation. It was difficult for me to choose one word that seems on paper incorrect or counter-intuitive but I settled with "limit" which was deemed as negative. Limits and limiting something or someone should not just be looked at as a negative word when limits can be very important and beneficial in a positive way.  

```{r}
bing <- get_sentiments("bing")

bing_word_counts_2022 <- unique_words_2022 |>
  inner_join(bing) |>
  count(word, sentiment, sort = TRUE)

bing_word_counts_2024 <- unique_words_2024 |>
  inner_join(bing) |>
  count(word, sentiment, sort = TRUE)

View(bing_word_counts_2022)
View(bing_word_counts_2024)
```

Gauging the sentiment of a word can be heavily dependent on the context, and as with other types of text analysis sometimes larger patterns are more meaningful than individual results. But the potential with text analysis is vast: knowing what words and phrases that public officials employ can be a way to evaluate their priorities, cohesiveness and tactics for persuading voters and their colleagues. And those words and phrases are data.
